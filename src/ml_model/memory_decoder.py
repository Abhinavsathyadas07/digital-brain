"""\nMemory Decoder ML Model\nLSTM-based neural network for predicting memory content from hippocampal signals\n"""\n\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nfrom typing import Tuple\n\n\nclass MemoryDecoderLSTM(nn.Module):\n    """\n    LSTM neural network for decoding memory from neural signals.\n    Predicts whether memory encoding was successful.\n    """\n    \n    def __init__(self, input_dim: int = 64, hidden_dim: int = 128, \n                 num_layers: int = 2, num_classes: int = 2):\n        super(MemoryDecoderLSTM, self).__init__()\n        \n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        \n        # LSTM layers\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, \n                           batch_first=True, dropout=0.3)\n        \n        # Attention mechanism\n        self.attention = nn.Linear(hidden_dim, 1)\n        \n        # Fully connected output\n        self.fc = nn.Sequential(\n            nn.Linear(hidden_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(64, num_classes)\n        )\n    \n    def forward(self, x):\n        # x shape: (batch, sequence_length, input_dim)\n        lstm_out, _ = self.lstm(x)\n        \n        # Apply attention\n        attention_weights = torch.softmax(self.attention(lstm_out), dim=1)\n        context = torch.sum(attention_weights * lstm_out, dim=1)\n        \n        # Classification\n        output = self.fc(context)\n        return output\n\n\nclass MemoryDecoder:\n    """\n    Wrapper class for training and inference with memory decoder.\n    """\n    \n    def __init__(self, input_dim: int = 64, device: str = 'cpu'):\n        self.device = torch.device(device)\n        self.model = MemoryDecoderLSTM(input_dim=input_dim).to(self.device)\n        self.scaler = StandardScaler()\n        self.trained = False\n    \n    def prepare_data(self, signals: np.ndarray, labels: np.ndarray, \n                    sequence_length: int = 100) -> Tuple[torch.Tensor, torch.Tensor]:\n        """\n        Prepare neural signals for LSTM input.\n        \n        Args:\n            signals: Shape (num_channels, num_samples)\n            labels: Binary labels (0=failed encoding, 1=successful)\n            sequence_length: Number of time steps per sequence\n            \n        Returns:\n            X_tensor, y_tensor for PyTorch training\n        """\n        # Reshape into sequences\n        num_sequences = signals.shape[1] // sequence_length\n        signals_reshaped = signals[:, :num_sequences * sequence_length].T\n        signals_reshaped = signals_reshaped.reshape(num_sequences, sequence_length, -1)\n        \n        # Normalize\n        orig_shape = signals_reshaped.shape\n        signals_flat = signals_reshaped.reshape(-1, signals_reshaped.shape[-1])\n        signals_normalized = self.scaler.fit_transform(signals_flat)\n        signals_normalized = signals_normalized.reshape(orig_shape)\n        \n        # Create labels for each sequence\n        labels_seq = np.repeat(labels[:num_sequences], 1)\n        \n        X_tensor = torch.FloatTensor(signals_normalized).to(self.device)\n        y_tensor = torch.LongTensor(labels_seq).to(self.device)\n        \n        return X_tensor, y_tensor\n    \n    def train(self, X: torch.Tensor, y: torch.Tensor, epochs: int = 50, \n             learning_rate: float = 0.001, batch_size: int = 32):\n        """\n        Train the memory decoder model.\n        """\n        criterion = nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n        \n        dataset = torch.utils.data.TensorDataset(X, y)\n        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n        \n        self.model.train()\n        for epoch in range(epochs):\n            total_loss = 0\n            for batch_X, batch_y in dataloader:\n                optimizer.zero_grad()\n                outputs = self.model(batch_X)\n                loss = criterion(outputs, batch_y)\n                loss.backward()\n                optimizer.step()\n                total_loss += loss.item()\n            \n            if (epoch + 1) % 10 == 0:\n                print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(dataloader):.4f}')\n        \n        self.trained = True\n    \n    def predict(self, signals: np.ndarray, sequence_length: int = 100) -> np.ndarray:\n        """\n        Predict memory encoding success from new signals.\n        \n        Returns:\n            Predicted probabilities for each class\n        """\n        if not self.trained:\n            raise ValueError("Model not trained. Call train() first.")\n        \n        # Prepare data\n        num_sequences = signals.shape[1] // sequence_length\n        signals_reshaped = signals[:, :num_sequences * sequence_length].T\n        signals_reshaped = signals_reshaped.reshape(num_sequences, sequence_length, -1)\n        \n        orig_shape = signals_reshaped.shape\n        signals_flat = signals_reshaped.reshape(-1, signals_reshaped.shape[-1])\n        signals_normalized = self.scaler.transform(signals_flat)\n        signals_normalized = signals_normalized.reshape(orig_shape)\n        \n        X_tensor = torch.FloatTensor(signals_normalized).to(self.device)\n        \n        # Predict\n        self.model.eval()\n        with torch.no_grad():\n            outputs = self.model(X_tensor)\n            probabilities = torch.softmax(outputs, dim=1)\n        \n        return probabilities.cpu().numpy()\n\n\nif __name__ == "__main__":\n    # Demo training\n    print("Creating synthetic training data...")\n    num_samples = 200\n    num_channels = 8\n    signal_length = 1000\n    \n    # Generate normal vs impaired signals\n    from src.signal_simulator.hippocampus_sim import HippocampusSimulator\n    \n    sim = HippocampusSimulator(num_channels=num_channels)\n    signals_list = []\n    labels_list = []\n    \n    for i in range(num_samples):\n        impaired = i >= num_samples // 2\n        _, sig = sim.generate_memory_encoding_pattern(duration=1.0, impaired=impaired)\n        signals_list.append(sig)\n        labels_list.append(0 if impaired else 1)\n    \n    signals = np.concatenate(signals_list, axis=1)\n    labels = np.array(labels_list)\n    \n    # Train decoder\n    decoder = MemoryDecoder(input_dim=num_channels)\n    X, y = decoder.prepare_data(signals, labels, sequence_length=100)\n    \n    print(f"Training data shape: {X.shape}")\n    print("Training memory decoder...")\n    decoder.train(X, y, epochs=30, batch_size=16)\n    \n    # Test prediction\n    _, test_sig = sim.generate_memory_encoding_pattern(duration=1.0, impaired=False)\n    predictions = decoder.predict(test_sig)\n    print(f"\nPrediction (normal signal): {predictions[0]}")\n    print(f"Predicted class: {'Successful encoding' if predictions[0][1] > 0.5 else 'Failed encoding'}")\n